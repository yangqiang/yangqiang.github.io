---
layout: post
title: "拨开 Spark OOM 迷雾：从 sortByKey 开始观察"
description: ""
category: Bigdata
tags: [Spark]
summary: 使用sortByKey()方法来构造OOM场景，从spark-shell的日志信息分析OOM出现的阶段及原因
---
{% include JB/setup %}

h2. 背景

spark 版本：1.0.0
集群设置：一主二从
Worker 配置：

| spark.executor.memory 1g | // Memory(Heap) usage limit for each worker node |
| spark.cores.max  32 |//  the maximum amount of CPU cores to request for the application from across the cluster (not from each machine) |
| spark.default.parallelism 2 | // Default number of tasks to use across the cluster for distributed shuffle operations (groupByKey, reduceByKey, etc) when not set by user. |

sort负载运行方式：

* spark-shell 中输入相关 scala 代码
* 输入数据规模：4.84GB
* scala 语句

<pre class="prettyprint java">
val lines = sc.textFile("/test/OS_ORDER_ITEM.txt")
val data_map = lines.map(line => {(line, 1)})
val result = data_map.sortByKey().map{line => line._1}
result.count
</pre>

h2. 运行阶段

* 启动 spark-shell 之后，输出信息显示 driver 端的 Memory Store 可支配的内存大小为：294.4 MB ，相关信息： 
*storage.BlockManagerInfo: Registering block manager sg201:45196 with 294.4 MB RAM*
每个 worker 节点可以用于 block manager 的内存为 588.8 MB ，相关信息： 
*Registering block manager sg211:52781 with 588.8 MB RAM* 
这个阶段完成的工作包括：
** 启动各项服务，初始化运行环境，如 akka 连接，创建本地数据存放目录
** 创建 SparkContext 对象 sc

* 第一步：从HDFS读数据到RDD，相关代码：
val data_malines = sc.textFile("/test/OS_ORDER_ITEM.txt", 8)
这一步，几乎是瞬间完成的，其实这条操作只是在 driver 上创建了一个 RDD 对象，数据并没有从 HDFS 复制到 RDD，毕竟单纯从磁盘 load 4.84 GB 数据到内存都需要若干秒钟的时间。重要日志信息：
<notextile> org.apache.spark.rdd.RDD[String] = MappedRDD[1] at textFile at <console>:12 </notextile>
* 第二步：转化RDD，相关代码：
val data_map = lines.map(line => {(line, 1)})
也是瞬间完成的，核心日志：
*<notextile> data_map: org.apache.spark.rdd.RDD[(String, Int)] = MappedRDD[2] at map at <console>:14 </notextile>*
* 第三步：排序，代码：
val result = data_map.sortByKey().map{line => line._1}
这一步会提交 Job ，相关信息如下：

<pre class="prettyprint java linenums">
14/07/17 16:45:21,936 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
14/07/17 16:45:21,936 WARN snappy.LoadSnappy: Snappy native library not loaded
14/07/17 16:45:21,953 INFO mapred.FileInputFormat: Total input paths to process : 1
14/07/17 16:45:21,976 INFO spark.SparkContext: Starting job: sortByKey at <console>:16
</pre>

Job ID 为 0 ，这一步还会分析 HDFS 上的文件，将文件划分为 78 个块，意味着需要 78 个 task 来完成排序任务（这 78 个 task 被放在 0.0 号 TaskSet 中，TaskSet 、 Stage 、 Job 概念相近，Each job is divided into “stages” (e.g. map and reduce phases)[3] ），最后返回的结果类型为 ResultTask ，相关日志如下：

<pre class="prettyprint java linenums">
scheduler.DAGScheduler: Got job 0 (sortByKey at <console>:16) with 78 output partitions (allowLocal=false)
</pre>

------

问题：这一步，装入RDD的是否只是 partition 信息，意味着磁盘的数据并没有装入内存？（全部装入内存也是不现实的，4.84 GB > 1.0 GB * 2 ）
答：确实并不会执行排序操作，而只是将此操作写入 dependency 然后创建了新的 RDD 对象：result。在我的博文： "Spark 执行步骤详解：以 sortByKey 为例":/bigdata/2014/07/21/spark-sort-precedures/

问题：如果执行 persist(MEMORY_ONLY) 应该会产生 OOM，这个操作会触发从 DISK 读入 Memory 的过程？
答：执行这行代码：val rm = result.persist(org.apache.spark.storage.StorageLevel.MEMORY_ONLY) ，没有出现 OOM 错误，因为没有 action 操作。

问题： lines 直接做 count 流程如何？
答：扫描各 partition 的数据，最后 driver 对返回的 ResultTask 中的计数做汇总，历时 4.4 秒。

------

* 第四步： *result.count* 。这一步会触发 OOM 场景。任务提交后，Job 获得的 ID 为 4 ，此 Job 依赖 Stage 5 （这个 Stage 做的是什么操作， sort 吗？）， Stage 5 在 70 秒之后成功结束，结果类型为 ShuffleMapTask （这种类型的 task 会将数据写入 buckets ）。接着， Stage 4 （这个 stage 又是做什么操作的？）开始执行，提交第 32 个 task 之后，出现错误信息。

<pre class="prettyprint java linenums">
14/07/18 10:20:21,035 INFO spark.MapOutputTrackerMasterActor: Asked to send map output locations for shuffle 0 to spark@sg204:42198
14/07/18 10:20:36,084 INFO scheduler.TaskSetManager: Starting task 4.0:32 as TID 422 on executor 0: sg211 (PROCESS_LOCAL)
14/07/18 10:20:36,084 INFO scheduler.TaskSetManager: Serialized task 4.0:32 as 4278 bytes in 0 ms
14/07/18 10:20:36,088 WARN scheduler.TaskSetManager: Lost TID 412 (task 4.0:22)
14/07/18 10:20:36,091 WARN scheduler.TaskSetManager: Loss was due to java.lang.OutOfMemoryError
java.lang.OutOfMemoryError: Java heap space
        at scala.collection.mutable.ResizableArray$class.ensureSize(ResizableArray.scala:99)
        ...
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        ...
        at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
        at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)
        ...
        at org.apache.spark.scheduler.Task.run(Task.scala:51)
        ...
        at java.lang.Thread.run(Thread.java:636)

</pre>

从第4行日志信息可以看出，worker 节点 sg211 执行第 22 号任务（TID为412）的时候出现了错误。
后来，又启动了新线程（TID为423）重新执行 22 号任务；紧接着，10号任务（也是分配给 sg211 执行的，笔者观测到任务是交替分配给各 worker 节点的）也失败了，报如下错：

<pre class="prettyprint java linenums">
14/07/18 10:20:42,011 INFO scheduler.TaskSetManager: *Starting task 4.0:22 as TID 423 on executor 0: sg211 (PROCESS_LOCAL)*
14/07/18 10:20:42,012 INFO scheduler.TaskSetManager: Serialized task 4.0:22 as 4278 bytes in 1 ms
14/07/18 10:20:42,012 WARN scheduler.TaskSetManager: *Lost TID 400 (task 4.0:10)*
14/07/18 10:20:42,014 WARN scheduler.TaskSetManager: Loss was due to java.lang.OutOfMemoryError
java.lang.OutOfMemoryError: GC overhead limit exceeded
        at java.lang.AbstractStringBuilder.<init>(AbstractStringBuilder.java:62)
        at java.lang.StringBuilder.<init>(StringBuilder.java:85)
        at java.io.ObjectInputStream$BlockDataInputStream.readUTFBody(ObjectInputStream.java:3015)
        ...
        at org.apache.spark.serializer.JavaDeserializationStream.readObject(JavaSerializer.scala:63)
        ...
        at org.apache.spark.InterruptibleIterator.foreach(InterruptibleIterator.scala:28)
        ...
        at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)
        at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)
        at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
        at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
14/07/18 10:20:46,363 INFO scheduler.TaskSetManager: Starting task 4.0:10 as TID 424 on executor 0: sg211 (PROCESS_LOCAL)
14/07/18 10:20:46,363 INFO scheduler.TaskSetManager: Serialized task 4.0:10 as 4278 bytes in 0 ms
</pre>

h3. 思考

# 这几个 Stage 的 Result 分别是什么类型？（参考 jerryshao 写的关于 spark scheduler 的博客）
# 为什么将 worker 节点最大使用的核数降低到 2 之后 OOM 问题会消失，核数设置为多大是 OOM 出现的临界点？（多核模式会加快内存的消耗速度，测试总并行度设置为9的时候仍会出现 OOM 错误）
# RDD.compute() 的原理？（生成 RDD 数据，比如 HadoopRDD.compute() 实现了从 HDFS 读取数据）
# 会不会出现这种情况：减少核数的方案与改变内存管理策略的方案最后性能相仿？
# Hadoop 默认 Block 大小为多大？（ 64 MB ， 4.84 * 1024 / 64 = 78 ，所以被划分为 78 个任务）
# 能否通过限制 Partition 数来控制 OOM 错误的发生？当 Partition 数小于 默认 Block 总数时，是否会被 Block 总数替代？（当 Partition 数小于 Block 数时，确实会被 Block 数替换，实验证明将 Partition 数设置得很大时，OOM 错误会消失）
# 将 RDD 设置为 DISK_ONLY 的存储模式，能否消除 OOM 问题？执行以下代码依然会有 OOM 问题。（默认的16核乘64MB刚好等于1GB，内存不够用）

<pre class="prettyprint scala">
val lines = sc.textFile("/test/OS_ORDER_ITEM.txt").persist(org.apache.spark.storage.StorageLevel.DISK_ONLY)
lines.map(line => {(line, 1)}).sortByKey().count
</pre>

#_ RDD 的 Iterator 与 BlockManager 之间是如何通信的？
RDD.iterator():

<pre class="prettyprint scala">
  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
    if (storageLevel != StorageLevel.NONE) {
      SparkEnv.get.cacheManager.getOrCompute(this, split, context, storageLevel)
    } else {
      computeOrReadCheckpoint(split, context)
    }
  }
</pre>

p(((. 可看出优先从 cacheManager 中取数据，这个 cacheManager 便是与 blockManager 打交道来存取数据的。当 StorageLevel 为 NONE 时，调用 RDD.computeOrReadCheckpoint 来获得 RDD 各 partition 数据。

#_ scala.collection.mutable.ResizableArray$class.ensureSize(ResizableArray.scala:99) 这里的代码做的什么操作，是否只会被 MemoryStore 调用？（日志里的这行错误并非cacheManager导致）
# Shuffle Reduce 与 Map 是同时启动，还是 Map 全部执行完才启动 Reduce ？

h2. 初步结论

对于 sortByKey 操作，其内存消耗量大概是： mem_needed = cores_assigned * partition_size
* cores_assigned 表示 worker 节点计算所使用的核数；
* partition_size 表示每个 partition 数据大小；

Spark 开发者也注意到这个问题，在 spark jira 中有一则 New Feature ： "Support external sorting for RDD#sortByKey()":https://issues.apache.org/jira/browse/SPARK-983?jql=text%20~%20%22sortByKey%22

OOM 的原因就是 mem_needed 大于剩余内存量，因此可以推测其他以 groupByKey 为代表的 shuffle 类操作的过程也是相似，能够成功 fetch 到数据，但是在对 fetch 到的数据做 merge 操作的时候，所使用的内存超过了剩余内存（Heap），因此失败。

jerryshao 对 shuffle 分析的文章[2] 中提到：

<pre>
那么Spark是否也有merge sort呢，还是以别的方式实现，下面我们就细细说明。

首先虽然Spark属于MapReduce体系，但是对传统的MapReduce算法进行了一定的改变。Spark假定在大多数用户的case中，shuffle数据的sort不是必须的，比如word count，强制地进行排序只会使性能变差，因此Spark并不在Reducer端做merge sort。既然没有merge sort那Spark是如何进行reduce的呢？这就要说到aggregator了。

aggregator本质上是一个hashmap，它是以map output的key为key，以任意所要combine的类型为value的hashmap。当我们在做word count reduce计算count值的时候，它会将shuffle fetch到的每一个key-value pair更新或是插入到hashmap中(若在hashmap中没有查找到，则插入其中；若查找到则更新value值)。这样就不需要预先把所有的key-value进行merge sort，而是来一个处理一个，省下了外部排序这一步骤。但同时需要注意的是reducer的内存必须足以存放这个partition的所有key和count值，因此对内存有一定的要求。

在上面word count的例子中，因为value会不断地更新，而不需要将其全部记录在内存中，因此内存的使用还是比较少的。考虑一下如果是group by key这样的操作，Reducer需要得到key对应的所有value。在Hadoop MapReduce中，由于有了merge sort，因此给予Reducer的数据已经是group by key了，而Spark没有这一步，因此需要将key和对应的value全部存放在hashmap中，并将value合并成一个array。可以想象为了能够存放所有数据，用户必须确保每一个partition足够小到内存能够容纳，这对于内存是一个非常严峻的考验。因此Spark文档中建议用户涉及到这类操作的时候尽量增加partition，也就是增加Mapper和Reducer的数量。

增加Mapper和Reducer的数量固然可以减小partition的大小，使得内存可以容纳这个partition。但是我们在shuffle write中提到，bucket和对应于bucket的write handler是由Mapper和Reducer的数量决定的，task越多，bucket就会增加的更多，由此带来write handler所需的buffer也会更多。在一方面我们为了减少内存的使用采取了增加task数量的策略，另一方面task数量增多又会带来buffer开销更大的问题，因此陷入了内存使用的两难境地。

为了减少内存的使用，只能将aggregator的操作从内存移到磁盘上进行，Spark社区也意识到了Spark在处理数据规模远远大于内存大小时所带来的问题。因此PR303提供了外部排序的实现方案，相信在Spark 0.9 release的时候，这个patch应该能merge进去，到时候内存的使用量可以显著地减少。
</pre>

h3. 如何解决 OOM 问题

* 减少核数，会导致系统 CPU 资源无法充分利用
* 增加并行度，reducer 数目会变大，shuffle 阶段不易 OOM
* 增加 partition 数， map 数目会变大，shuffle 阶段不容易发生 OOM

h2. 参考资料

# "Spark源码分析之-scheduler模块":http://jerryshao.me/architecture/2013/04/21/Spark源码分析之-scheduler模块/
# "详细探究Spark的shuffle实现":http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/
# "Job Scheduling":http://spark.apache.org/docs/1.0.0/job-scheduling.html

