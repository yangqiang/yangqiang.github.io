---
layout: post
title: "Spark 执行步骤详解：以 sortByKey 为例"
description: ""
category: BigData
tags: [Spark]
summary: Spark OOM 分析的第二篇，结合 Spark 1.0.0 的代码来分析 Spark 处理任务的流程
---
{% include JB/setup %}

h2. 引言

Spark 做排序一般都使用 sortByKey 方法，笔者观测到 Spark 对超出内存大小若干倍的数据做排序时，容易引发 OOM 的错误。本文主要针对一个 sortByKey 的例子，结合 Spark-1.0.0 的源代码对排序过程展开细粒度的分析。

h2. 相关代码

<pre class="prettyprint java">
val lines = sc.textFile("/test/OS_ORDER_ITEM.txt")
val data_map = lines.map(line => {(line, 1)})
val result = data_map.sortByKey().map{line => line._1}
result.count
</pre>

h2. 线索：一个典型的错误日志

<pre class="prettyprint java linenums">
java.lang.OutOfMemoryError: Java heap space
	at scala.collection.mutable.ResizableArray$class.ensureSize(ResizableArray.scala:99)
	...
	at org.apache.spark.InterruptibleIterator.to(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toBuffer(TraversableOnce.scala:265)
	at org.apache.spark.InterruptibleIterator.toBuffer(InterruptibleIterator.scala:28)
	at scala.collection.TraversableOnce$class.toArray(TraversableOnce.scala:252)
	at org.apache.spark.InterruptibleIterator.toArray(InterruptibleIterator.scala:28)
	at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62)
	at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:61)
	at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
	at org.apache.spark.rdd.RDD$$anonfun$12.apply(RDD.scala:559)
	at org.apache.spark.rdd.MapPartitionsRDD.compute(MapPartitionsRDD.scala:35)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.rdd.MappedRDD.compute(MappedRDD.scala:31)
	at org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:262)
	at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
	at org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:111)
	at org.apache.spark.scheduler.Task.run(Task.scala:51)
	at org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
</pre>

第 21 行至第 18 行是 worker 处理 driver 发送来的 task 的过程。

* ResultTask.runTask

<pre class="prettyprint scala">
  override def runTask(context: TaskContext): U = {
    metrics = Some(context.taskMetrics)
    try {
      func(context, rdd.iterator(split, context))
    } finally {
      context.executeOnCompleteCallbacks()
    }
  }
</pre>

这里的 func 函数是由 DAGScheduler 把 *ActiveJob* 对象的对应属性传入 ResultTask 的。这个方法只会在 Worker 节点上被调用，用于完成对 RDD 的操作。

而 ActiveJob 在 DAGScheduler.handleJobSubmitted 中被创建，func 的类型为： func: (TaskContext, Iterator[_]) => _,
handleJobSubmitted 方法会被 DAG Scheduler 事件处理主循环调用： DAGSchedulerEventProcessActor.receive()

<pre class="prettyprint">
case JobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite, listener, properties) =>
      dagScheduler.handleJobSubmitted(jobId, rdd, func, partitions, allowLocal, callSite,
        listener, properties)
</pre>

RDD.count 第 847 行有： *sc.runJob(this, Utils.getIteratorSize _).sum* ，这里的 Utils.getIteratorSize 便是一个 func
这么看来 driver 上的 RDD 才是命令的发出者， sc.runJob 触发了一系列操作。

ResultTask 对象包含了 TaskContext

h3. 关键步骤

* 第 28 行： at org.apache.spark.rdd.RDD.iterator(RDD.scala:229)
* at org.apache.spark.rdd.OrderedRDDFunctions$$anonfun$sortByKey$1.apply(OrderedRDDFunctions.scala:62) // 匿名函数

h2. 走进源代码

h3. OrderedRDDFunctions.sortByKey()

这行代码是在 Driver 节点上执行的，第 59 行代码会触发提交 Job ，过程中会对 RDD 做采样，定出 partition 的上界。（后面会有更详细的分析）

<pre class="prettyprint scala linenums:58">
  def sortByKey(ascending: Boolean = true, numPartitions: Int = self.partitions.size): RDD[P] = {
    val part = new RangePartitioner(numPartitions, self, ascending)
    val shuffled = new ShuffledRDD[K, V, P](self, part) // 这一步是会生成新的 RDD
    shuffled.mapPartitions(iter => {	// 对每个 partition 内的数据做排序，返回排序后的 RDD ， Driver 端执行
      val buf = iter.toArray
      if (ascending) {
        buf.sortWith((x, y) => ordering.lt(x._1, y._1)).iterator
      } else {
        buf.sortWith((x, y) => ordering.gt(x._1, y._1)).iterator
      }
    }, preservesPartitioning = true)
  }
</pre>

第 62 行代码将 iter 转换为数组，假设同时有 16 个 task 执行，那么将消耗 16 * 64 MB = 1024 MB 内存。
实际测试过程中发现，当单个节点核数设置为 3 时，即此操作使用 192 MB 内存时，就会引发 OOM 错误。
1GB Heap 有 588MB 被分配给 BlockManager ，还有其它的对象占用了 Heap ，导致排序操作执行时 Heap 空间不够用。

data_map （类型为 MappedRDD ）并没有 sortByKey() 方法，通过 scala 的隐式转换（ implicit conversion ）创建了 OrderedRDDFunctions 对象，然后调用此方法。代码位置： spark.SparkContext

<pre class = "prettyprint scala">
  implicit def rddToPairRDDFunctions[K, V](rdd: RDD[(K, V)])
      (implicit kt: ClassTag[K], vt: ClassTag[V], ord: Ordering[K] = null) = {
    new PairRDDFunctions(rdd)
  }
</pre>

但是，为什么执行到这一行代码的时候，会提交 job 触发 worker 节点工作？（ sortByKey() 不属于 action ）
val result = data_map.sortByKey().map{line => line._1}

重读了 Spark 的论文，确认了 sortByKey 不属于 action 操作。
"jerryshao 的博客":http://jerryshao.me/architecture/2013/04/21/Spark源码分析之-scheduler模块/ 在“ Job 的生与死”小节也提到类似问题，不过未加解释，实际上，reduceByKey 操作并不会触发 Job。

bq. 在对RDD的count()和reduceByKey()操作都会调用SparkContext的runJob()来提交job

后来，在 spark user list 里找到了一个相关问题： "non-lazy execution of sortByKey?":http://apache-spark-user-list.1001560.n3.nabble.com/non-lazy-execution-of-sortByKey-td3835.html
Spark JIRA 里面也标注出这是一个 Bug:  "sortByKey() launches a cluster job when it shouldn't":https://issues.apache.org/jira/browse/SPARK-1021?jql=text%20~%20%22sortByKey%22
提交 Job 的原因是，创建 RangePartitioner 的时候会调用 rdd.sample 方法计算 Partition 上界，而 rdd.sample 方法是会触发 Job 提交的，详见下面对 RangePartitioner 源代码的分析。

h4. RangePartitioner

这行代码：

<pre>
val part = new RangePartitioner(numPartitions, self, ascending)
</pre>

会调用以下代码来创建 rangeBounds 数组类型的变量，该数组的每个元素记录了新 RDD 每个 partitoner （前 n-1 个）的上限 key 值：

<pre>
val rddSample = rdd.sample(false, frac, 1).map(_._1).collect().sorted
</pre>

这里面有 action 操作：rdd.sample。

h4. ShuffledRDD

其 compute() 方法会调用 ShuffleFetcher.fetch() 返回数据。

h4. ShuffleMapTask.runTask()

这一步实现了数据分发的逻辑：

<pre class="prettyprint scala">
  override def runTask(context: TaskContext): MapStatus = {
    val numOutputSplits = dep.partitioner.numPartitions
    metrics = Some(context.taskMetrics)

    val blockManager = SparkEnv.get.blockManager
    val shuffleBlockManager = blockManager.shuffleBlockManager
    var shuffle: ShuffleWriterGroup = null
    var success = false

    try {
      // Obtain all the block writers for shuffle blocks.
      val ser = Serializer.getSerializer(dep.serializer)
      shuffle = shuffleBlockManager.forMapTask(dep.shuffleId, partitionId, numOutputSplits, ser)

      // Write the map output to its associated buckets.
      for (elem <- rdd.iterator(split, context)) { // 以 sort 为例，这一行扫描每行文本
        val pair = elem.asInstanceOf[Product2[Any, Any]]
        val bucketId = dep.partitioner.getPartition(pair._1) // 当前行内容所属 partition
        shuffle.writers(bucketId).write(pair)
      }
    ...
  }
</pre>

*val bucketId = dep.partitioner.getPartition(pair._1)* 这行代码根据 RDD 的元素值计算其所属 Partition。

h4. RangePartitioner.getPartition

使用二分查找找到 key 所属的 partition 。

<pre class="prettyprint scala">
  def getPartition(key: Any): Int = {
    val k = key.asInstanceOf[K]
    var partition = 0
    if (rangeBounds.length < 1000) {
      // If we have less than 100 partitions naive search
      while (partition < rangeBounds.length && ordering.gt(k, rangeBounds(partition))) {
        partition += 1
      }
    } else {
      // Determine which binary search method to use only once.
      partition = binarySearch(rangeBounds, k)
      // binarySearch either returns the match location or -[insertion point]-1
      if (partition < 0) {
        partition = -partition-1
      }
      if (partition > rangeBounds.length) {
        partition = rangeBounds.length
      }
    }
    if (ascending) {
      partition
    } else {
      rangeBounds.length - partition
    }
  }
</pre>

h4. mapPartitions()

方法功能：对每个 Partition 根据传入的方法做 map 操作。
shuffled.mapPartitions 这一句会调用 new MapPartitionsRDD(this, sc.clean(func), preservesPartitioning)
*很多转化过程都是通过新建 MapPartitionsRDD 来记录的， Spark 便是通过这些方法引用来实现 lazy 模式。*

<pre class="prettyprint scala">
  def mapPartitions[U: ClassTag](
      f: Iterator[T] => Iterator[U], preservesPartitioning: Boolean = false): RDD[U] = {
    val func = (context: TaskContext, index: Int, iter: Iterator[T]) => f(iter)
    new MapPartitionsRDD(this, sc.clean(func), preservesPartitioning)
  }
</pre>

h5. 计算的时候是如何根据 RangePartitioner 的 rangeBounds 来取数据的？

RangePartitioner 实现的相关代码：

<pre class="prettyprint scala linenums:92">
class RangePartitioner[K : Ordering : ClassTag, V](
    partitions: Int,
    @transient rdd: RDD[_ <: Product2[K,V]],
    private val ascending: Boolean = true)
  extends Partitioner {

  private val ordering = implicitly[Ordering[K]]

  // An array of upper bounds for the first (partitions - 1) partitions
  private val rangeBounds: Array[K] = {
    ...
  }

  ...
}
</pre>

h3. RDD.count()

action 触发了 RDD 内部的计算，意味着所有的负载计算在那时执行，之前所做的都只是用 function 记录各种转化操作。

<pre class="prettyprint scala">
  /**
   * Run a function on a given set of partitions in an RDD and pass the results to the given
   * handler function. This is the main entry point for all actions in Spark. The allowLocal
   * flag specifies whether the scheduler can run the computation on the driver rather than
   * shipping it out to the cluster, for short actions like first().
   */
  def runJob[T, U: ClassTag](
      rdd: RDD[T],
      func: (TaskContext, Iterator[T]) => U,
      partitions: Seq[Int],
      allowLocal: Boolean,
      resultHandler: (Int, U) => Unit) {
    if (dagScheduler == null) {
      throw new SparkException("SparkContext has been shutdown")
    }
    val callSite = getCallSite
    val cleanedFunc = clean(func)
    logInfo("Starting job: " + callSite)
    val start = System.nanoTime
    dagScheduler.runJob(rdd, cleanedFunc, partitions, callSite, allowLocal,
      resultHandler, localProperties.get)
    logInfo("Job finished: " + callSite + ", took " + (System.nanoTime - start) / 1e9 + " s")
    rdd.doCheckpoint()
  }
</pre>

这是分发任务的入口，DAGScheduler 将 Job 按 partition 数目划分为 task 交给 worker 执行。
worker 拆开 task 执行的时候（scheduler.ResultTask/ShuffleMapTask.runTask() 方法）会调用 RDD.iterator 来读取 RDD 的数据。（如果没有获得，就会去计算以生成数据：getOrCompute）

h3. RDD.iterator

<pre class="prettyprint scala">
  final def iterator(split: Partition, context: TaskContext): Iterator[T] = {
    if (storageLevel != StorageLevel.NONE) {
      SparkEnv.get.cacheManager.getOrCompute(this, split, context, storageLevel)
    } else {
      computeOrReadCheckpoint(split, context)
    }
  }
</pre>

computeOrReadCheckpoint() 方法是数据加载的秘密，该方法最后会调用 compute 方法。 sortByKey() 方法里面先将 RDD 转化为 ShuffledRDD ，其 compute 方法如下：

<pre class="prettyprint scala">
  override def compute(split: Partition, context: TaskContext): Iterator[P] = {
    val shuffledId = dependencies.head.asInstanceOf[ShuffleDependency[K, V]].shuffleId
    val ser = Serializer.getSerializer(serializer)
    SparkEnv.get.shuffleFetcher.fetch[P](shuffledId, split.index, context, ser)
  }
</pre>

BlockStoreShuffleFetch 实现了 ShuffleFetcher 接口的 fetch() 方法，融合各 worker 上 bucket 中的数据。

<pre class="prettyprint scala">
  override def fetch[T](
      shuffleId: Int,
      reduceId: Int,
      context: TaskContext,
      serializer: Serializer)
    : Iterator[T] =
  {

    logDebug("Fetching outputs for shuffle %d, reduce %d".format(shuffleId, reduceId))
    val blockManager = SparkEnv.get.blockManager

    val startTime = System.currentTimeMillis
    val statuses = SparkEnv.get.mapOutputTracker.getServerStatuses(shuffleId, reduceId)
    logDebug("Fetching map output location for shuffle %d, reduce %d took %d ms".format(
      shuffleId, reduceId, System.currentTimeMillis - startTime))

    val splitsByAddress = new HashMap[BlockManagerId, ArrayBuffer[(Int, Long)]]
    for (((address, size), index) <- statuses.zipWithIndex) {
      splitsByAddress.getOrElseUpdate(address, ArrayBuffer()) += ((index, size))
    }

    val blocksByAddress: Seq[(BlockManagerId, Seq[(BlockId, Long)])] = splitsByAddress.toSeq.map {
      case (address, splits) =>
        (address, splits.map(s => (ShuffleBlockId(shuffleId, s._1, reduceId), s._2)))
    }

    def unpackBlock(blockPair: (BlockId, Option[Iterator[Any]])) : Iterator[T] = {
      val blockId = blockPair._1
      val blockOption = blockPair._2
      blockOption match {
        case Some(block) => {
          block.asInstanceOf[Iterator[T]]
        }
        case None => {
          blockId match {
            case ShuffleBlockId(shufId, mapId, _) =>
              val address = statuses(mapId.toInt)._1
              throw new FetchFailedException(address, shufId.toInt, mapId.toInt, reduceId, null)
            case _ =>
              throw new SparkException(
                "Failed to get block " + blockId + ", which is not a shuffle block")
          }
        }
      }
    }

    val blockFetcherItr = blockManager.getMultiple(blocksByAddress, serializer)
	...
  }
</pre>

最后将 RDD 转化为 MapPartitionsRDD ，其 compute 方法如下：

<pre class="prettyprint scala">
  override def compute(split: Partition, context: TaskContext) =
    f(context, split.index, firstParent[T].iterator(split, context))
</pre>

h2. Shuffle 过程分析

之前的理解是错误的：

<pre>
sortByKey() 分为两个步骤：

1. 对每个 partition 做排序
2. 将所有排好序的 partition 合并，形成一个新的 RDD
</pre>

当时未解决的问题：

<pre>
第二步必须通过 action 会触发，其执行细节是怎样的？
1. 在 driver 还是 worker 上做合并？（ worker ）
2. 合并排序结果时，数据通信是怎么做的？
3. 怎么到达 shuffle 阶段？（ ExternalAppendOnlyMap ，在 sg108 的日志文件 /home/jonny/java_pid748.hprof 中并未找到这个实例，也没有 Aggregator ）

PairedRDDFunctions 使用的才是 Aggregator 。
</pre>

实际流程应该是这样：

# 对原始 RDD 做 sample（采样），计算每个新的 partition 的上界，即构造出一个 RangePartitioner 对象；
# 使用 RangePartitioner 构造新的 ShuffledRDD；
# 计算过程中分为两步：1）Map 阶段，按“桶排序”的方法，对每个 partition 的数据进行处理，根据数据的值分发到不同的 bucket；2）Reduce 阶段，从每个 worker 上的 bucket 文件中取出数据，放入每个 partition；
# 对每个 partition 内的数据做排序，由于 RangePartitioner 保证 partition 之间整体有序，因此不需要再做归并排序。

整个过程正如 jerryshao 的 "《详细探究Spark的shuffle实现》":http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation 博客中的一张图所示：
!/pics/spark-shuffle.png!

以 sortByKey 为例， shffule 过程如下图所示：
!/pics/a-spark-shuffle-example-sort.png!
图中红色的数字表示每个 partition 的上界。


h2. 小结

Spark 任务执行过程小结：

* 集群启动的时候各 worker 节点会初始化 SparkContext 对象；
* 用户提交的 action 操作，如 count, collect 会被 Driver 发送到 Worker 执行；
* 收到 task 之后， SparkContext 对象根据任务去创建 RDD 并进行计算；
* Driver 从 Worker 取得数据，做简单处理计算得到最终结果。

以仓库管理类比：（1个主管，3个手下，3个仓库）

* 好比仓库主管给每个手下一把分库的钥匙；
* 主管将任务切分为 1 2 3 三块，分别分配给三个手下去执行，一个好的主管，会根据手下离仓库的距离来分配任务，使每个手下能就近取货进行工作，主管以短信的方式发送任务，比如“拆卸”、“烘干”、“包装”，三个手下按步骤一一执行任务；
* 手下执行任务的时候会根据任务编号从仓库取出货物进行处理；
* 有些任务需要协同完成，比如主管想把一批货里面货物按生产日期重新排列，这就要求各手下发短信协调完成。这种 shuffle 类的计算，需要为分仓库之间建立传送带，保证每两个仓库之间都能相互传送货物。

RDD 的变迁过程都是在 Driver 上记录的，那么 Worker 上的 RDD 对象是如何创建的呢？（直观猜测是 DAGScheduler 将封装好的 task 发送给 Worker ，Worker 拆开 task 可以看到里面的 RDD 信息，基于 scala 中 akka.actor 技术实现，天然支持分布式编程）
更多细节可参考 CSDN 博客： "Spark技术内幕：Client，Master和Worker 通信源码解析":http://blog.csdn.net/anzhsoft/article/details/30802603