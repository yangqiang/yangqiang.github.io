---
layout: post
title: "Spark 开发者动态"
description: ""
category: bigdata
tags: [Spark]
summary: OOM 问题，RDD checkpoint 技术，一些重要的 Spark 配置参数。
---
{% include JB/setup %}

h2. 动态

h3. OOM 问题

最近一个月以来，我主要在分析 Spark OOM 的原因，其实在此之前已经有一些相关的分析工作。我梳理了这些工作，并结合自己的分析总结了 OOM 发生的原因：
* 直接将大量数据读入内存。RDD 的 shuffle 类操作会将 partition 数据全部加载到内存中，如果提交 Spark Application 时，设置每个 Worker 节点最大使用核数为 n（通过参数 spark.cores.max 设置集群总核数，含义是同时运行的最大任务数），每个 partition 数据大小为 x，那么 shuffle 操作的 Map 阶段将消耗 n*x 大小的内存。Spark-1.0.0 版本已经为 reduceByKey 操作实现了外部排序以减小内存开销，但是 sortByKey 的外部排序还未实现，目前 Spark 的开发者已经 开始这一块的工作，可参考 "这个链接":https://issues.apache.org/jira/browse/SPARK-983?jql=text%20~%20%22sortByKey%22 。为减少这类 shuffle 操作引发的 OOM 错误，用户可以在创建 RDD 时将 partition 数设置得大一些，这样 n*x 中的 x 会变小。接下来，我会讲讲这个方法的副作用。
* 文件 buffer 过大。Spark-1.0.0 中在执行 shuffle 类操作时，会根据 map 数（M 即 partition 数）和 reduce 数（R，通过 spark.default.parallelism 参数设置）来生成 M*R  个 bucket 存放 shuffle 的输出数据，Spark-1.0.0 中的 BlockManager 模块会为每个 bucket 开启一个 100KB（可以通过 spark.shuffle.file.buffer.kb 配置）大小的 buffer，用来提高将 RDD 数据从内存导入导出到磁盘的速度。因此，增加 partition 在减小 n*x 中的 x 的同时，也增大了 M，即增大了 M，文件 buffer 开销变大。Spark-1.0.0 可以通过配置 spark.shuffle.consolidateFiles 参数为 true 把 bucket 开销降低至 n * R。

h3. 容错的效率

Spark 的 RDD checkpoint 技术可以将一些关键的中间数据备份，以方便错误发生时快速恢复数据，而不需要从头开始计算。从 "Provide memory-and-local-disk RDD checkpointing":https://issues.apache.org/jira/browse/SPARK-1855 这个 JIRA Issue 可以看出，Spark 当前使用 HDFS 来备份 RDD 的中间数据，开发者也正在改进数据备份策略，以支持将数据能备份在内存或者磁盘中。

这一块其实还有一些工作可以做，考虑 PageRank 的计算过程例子：
!/pics/spark-pagerank-lineage.png!

如果 ranks2 已经备份好，那么之前依赖的 ranks0、contribs0、ranks1、contribs1 都可以从内存中释放掉。可以考虑为“冷热数据”制定不同的策略。

h3. 高效传输

为了提高数据传输的效率，Spark 使用了 Tachyon 技术来共享数据文件，并使用了序列化技术来加速数据传输。

h2. 问题

* Spark 中 reduce 数是如何确定的？
** cores, executors, map, reduce, partition, parallelism 在 Spark 中分别具有什么含义？
*** *spark.default.parallelism*: Default number of tasks to use across the cluster for *distributed shuffle operations* (groupByKey, reduceByKey, etc) when not set by user. Local mode: number of cores on the local machine; Mesos fine grained mode: 8; Others: total number of cores on all executor nodes or 2, whichever is larger. 对应 Reduce 数。
*** spark.task.cpus: Number of cores to allocate for each task. 默认值是1，这里的 core 是指操作系统的“核”
*** *spark.cores.max*：When running on a standalone deploy cluster or a Mesos cluster in "coarse-grained" sharing mode, the maximum amount of CPU cores to request for the application from across the cluster (not from each machine). If not set, the default will be spark.deploy.defaultCores on Spark's standalone cluster manager, or infinite (all available cores) on Mesos. 对应 Map 数。
*** SPARK_WORKER_MEMORY：Total amount of memory to allow Spark applications to use on the machine, e.g. 1000m, 2g (default: total memory minus 1 GB); note that each application's individual memory is configured using its spark.executor.memory property.
*** SPARK_DAEMON_MEMORY：Memory to allocate to the Spark master and worker daemons themselves (default: 512m).
*** spark-submit 以 yarn 方式提交时的参数含义， "参考资料":http://stackoverflow.com/questions/24622108/apache-spark-the-number-of-cores-vs-the-number-of-executors
**** ==--==executor-cores：每个worker使用的核数。 "参考资料：Job Scheduling":http://spark.apache.org/docs/1.0.0/job-scheduling.html
**** ==--==num-executors：worker 数
从 "这个帖子":http://apache-spark-user-list.1001560.n3.nabble.com/Number-of-partitions-and-Number-of-concurrent-tasks-td10984.html#a11067 可以看出，Spark 中的 core 的总数决定了同时启动的任务数的最大值。
* Spark 是如何根据 reducer 数目来分发任务的，即每个 reducer 处理的 partition 数是如何确定的？
这个问题的提法不够准确，map 阶段会根据 reducer 个数生成 bucket，意味着一个 partition 中的数据可能被分发到多个 reducer，根据 partition 中数据内容，有些 reducer 的 bucket 可能为空。
* Worker 节点是否做完所有的 map 任务后才会开始做 reduce 任务，Spark源代码中是如何体现这一块的？
"这个页面":http://spark.apache.org/docs/1.0.0/job-scheduling.html 提到“Each job is divided into “stages” (e.g. map and reduce phases)”，可见在 map 和 reduce 是两个不同的阶段。
* RDD.collect 也很容易 OOM？
SPARK_DAEMON_MEMORY 默认是512 MB，意味着 Driver 默认最大可使用内存为 512 MB，而 RDD.collect 一般返回非常大的数据，因此很容易发生 OOM。
* Spark-1.0.0 中有没有自动做 checkpoint 的？
做了。在 job 执行完成后，生成的 RDD 会调用 checkpoint 函数。详情参考博客《RDD 的 checkpoint 技术》

h2. 参考

# "详细探究Spark的shuffle实现":http://jerryshao.me/architecture/2014/01/04/spark-shuffle-detail-investigation/
# "Resilient Distributed Datasets: A Fault-Tolerant Abstraction for In-Memory Cluster Computing":http://www.cs.berkeley.edu/~matei/papers/2012/nsdi_spark.pdf
# Spark 官方调优手册中推荐阅读 Oracle 官方提供的 JVM GC 调优文档 "Java SE 6 HotSpot[tm] Virtual Machine Garbage Collection Tuning":http://www.oracle.com/technetwork/java/javase/gc-tuning-6-140523.html